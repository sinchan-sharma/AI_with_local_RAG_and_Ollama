# Evaluating LangSmith Traces for Various Queries

Below is a brief summary and evaluation of the outputs generated for various question and summarization tasks, using screenshots from the LangSmith UI for tracing chains. These screenshots can be found in the `screenshots` folder. The discussion below is specifically for the unsuccessful traces in terms of retrieved content and LLM outputs.

## Unsuccessful LangChain Traces:

### Trace 1

In the `screenshots` folder, you'll see two images showing some queries and their traces that were unsuccessful in some way. For example, looking at the screenshot `unsuccessful_qa_trace_1`, the question asked was "Why is the new Congress energy bill bad?". This question stems fromt the `climate_change.html` file, which takls about a new budget bill recently passed by Congress and how it relates to energy demands. Since this document was used in the vector database, I expected that some relevant chunk embeddings would be retrieved and passed to the LLM as context, however, looking at the screenshot, we can see that this wasn't thae case. Instead the retrieved embeddings are from other sources, particularly from a different document on 1:1 technology and its implementation in classrooms. As a result the generated response is based on the wrong document and is irrelevant to the original question. Debugging this involved taking a closer look at my overall pipeline, specifically my custom function to retrieve the most similar embeddings and pass it along with the query to the LLM. The mistake here was that I was always extracting embeddings from the wrong vector store. Because I had to use two different embedding models, one API-based and the other using Hugging Face, I created two separete vector stores within ChromaDB, to ensure that there were no potential issues later on with poor retrieval wquality due to mixing-and-matching of different embeddimg models. Specifically all PDF files, which fell under the **Technology** category, I used Google embeddings and stored in a vector store, and all other non-PDF files I used Hugging Face embeddings and stored in a separate vector store.

Because the PDF-only vector store was always being used for embedding retrieval, no matter which question I asked related to the specific climate change article (which again is an HTML file), the relevant chunks/embeddings would never be retrieved, meaning there would be very little relevant context to pass with the query to the LLM. Fixing this was simple, as it involved adjusting which vector store would be accessed, depending on whether the user query was related to a PDF file or a non-PDF file, as opposed to accidently keeping it fixed no matter what.

### Trace 2

For this trace, this involved asking a question completely unrelated to any of the documents that were used for chunking and embedding into the vector stores, in order to evaluate the LLM response. Looking at the screenshot `unsuccessful_qa_trace_2`, we can see the user query is "What is the best red wine to pair with a medium-rare steak?", as well as the LLM response. When evaluating the LangSmith UI, we can see two things. First, the retrieved chunks is a table of contents page from a PDF file called `machine_learning_tutorial`. Given that none of the documents used for building the vector store had anything to do with food or beverages in any was, there was very little provided context for the LLM to work with, which it mentions as such in its response. That being said, the response itself is quite interesting. Rather than outright saying there wasn't enough relevant context to work with and give an answer, it instead talks about the provided context (a table of contents for a machine learning tutorial) and then uses its own internal knowledge to talk about recommended wine types for a medium-rare steak. The most intersting thing however, is the fact that toward the end, it attempts to make a connection between machine learning models and wine pairings with steak. This can be seen under the `**Connection to the Provided Context:**` section.

To fix this, I had to tweak my prompt templates a little, and essentially tell the LLM that if no relevant context is able to be retrieved for a particular query, to clearly tell the user that no information is available for that specific question, and to consider asking a different question instead and/or checking the document collection to see what sorts of topics are covered.
