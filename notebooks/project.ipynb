{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c3233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, JSONLoader, BSHTMLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7fd22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file variables\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "LANGSMITH_TRACING = os.getenv(\"LANGSMITH_TRACING\")\n",
    "LAMGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "LAMGSMITH_ENDPOINT = os.getenv(\"LAMGSMITH_ENDPOINT\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please set the GOOGLE_API_KEY in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "folder_path = \"../documents\"\n",
    "persist_directory = \"../chroma_db\"\n",
    "collection_name = \"all_documents\"\n",
    "chunk_size = 600\n",
    "chunk_overlap = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74393bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up old vector store and start fresh, only run this code if necessary\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instructions for dynamic prompting\n",
    "instructions = {\n",
    "    \"Factual\": \"\"\"\n",
    "    Your task is to answer the question based ONLY on the provided context. \n",
    "    Extract specific, accurate information directly from the text. \n",
    "    If the answer is not found, clearly state that the information is unavailable. \n",
    "    Provide a complete and well-formed answer in full sentences.\n",
    "    \"\"\",\n",
    "    \"Interpretive\": \"\"\"\n",
    "    Your task is to provide a thoughtful interpretive answer by synthesizing the information in the provided context. \n",
    "    Draw meaningful connections, explain implications, and summarize broader themes or significance. \n",
    "    Make your answer clear, coherent, and complete, even if the information is scattered.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "## Classification prompt to detect question type using few-shot examples\n",
    "question_classifier_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"\n",
    "You are a classifier that determines whether a user question is Factual or Interpretive.\n",
    "\n",
    "- A *Factual* question asks for specific information found directly in a document (e.g., names, dates, techniques, facts).\n",
    "- An *Interpretive* question asks for broader meaning, implications, or synthesis (e.g., summaries, themes, significance).\n",
    "\n",
    "Respond with only the single word: Factual or Interpretive.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Question: What year was Nikola Tesla born?\n",
    "Classification: Factual\n",
    "\n",
    "Question: What is this paper mainly about?\n",
    "Classification: Interpretive\n",
    "\n",
    "Question: Who are the key figures mentioned in this research?\n",
    "Classification: Factual\n",
    "\n",
    "Question: How does this paper relate to broader trends in machine learning?\n",
    "Classification: Interpretive\n",
    "\n",
    "Now classify the following question:\n",
    "Question: {query}\n",
    "Classification:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "## Add a topic classifier to detect question topic using few-shot examples\n",
    "topic_classifier_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"\n",
    "You are a classifier that assigns a topic label to a question. \n",
    "Choose the single most relevant topic from this exact list:\n",
    "\n",
    "- Technology\n",
    "- People\n",
    "- Science\n",
    "- Literature\n",
    "- Other\n",
    "\n",
    "Respond with ONLY the topic name (one of the above) and nothing else.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Question: What are common techniques used in machine learning?\n",
    "Topic: Technology\n",
    "\n",
    "Question: What was Alan Turing's contribution to computer science?\n",
    "Topic: People\n",
    "\n",
    "Question: What are some of the impacts of climate change?\n",
    "Topic: Science\n",
    "\n",
    "Question: What are some popular books published after 2000?\n",
    "Topic: Literature\n",
    "\n",
    "Question: What is a good cheese to pair with red wine?\n",
    "Topic: Other\n",
    "\n",
    "Now classify this question:\n",
    "Question: {query}\n",
    "Topic:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "## Base template for passing in instructions, context, and a user question\n",
    "base_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    \\\"\\\"\\\"\n",
    "    You are an intelligent document assistant.\n",
    "    {instruction}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: \n",
    "    {question}\n",
    "\n",
    "    Answer in complete sentences, using ONLY the information given in the context. \n",
    "    If the context does not contain the answer, say so clearly.\n",
    "\n",
    "    Answer:\n",
    "    \\\"\\\"\\\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45b2b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Ollama LLM (local model)\n",
    "llm = OllamaLLM(model=\"gemma3\")\n",
    "\n",
    "## Initialize chain for question classification (factual or interpretive)\n",
    "question_classifier_chain = question_classifier_prompt | llm | StrOutputParser()\n",
    "## Initialize chain for question topic classification (based on the topic classifier prompt above)\n",
    "topic_classifier_chain = topic_classifier_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign each file/document to a particular topic based on extension\n",
    "def assign_topic(path):\n",
    "    if path.endswith(\".pdf\"):\n",
    "        return \"Technology\"\n",
    "    elif path.endswith(\".txt\"):\n",
    "        return \"People\"\n",
    "    elif path.endswith(\".html\"):\n",
    "        return \"Science\"\n",
    "    elif path.endswith(\".json\"):\n",
    "        return \"Literature\" \n",
    "    else:\n",
    "        return f\"Unknown: This file type is unsupported {path}, so a topic was not assigned.\"\n",
    "\n",
    "## Split each document type into chunks\n",
    "def load_and_split_file(path, splitter):\n",
    "    should_split = True # All documents other than JSON files should be split/chunked\n",
    "                        # using a recursive character splitter \n",
    "    if path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(path)\n",
    "    elif path.endswith(\".txt\"):\n",
    "        loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    elif path.endswith(\".html\"):\n",
    "        loader = BSHTMLLoader(path, open_encoding=\"utf-8\")\n",
    "    elif path.endswith(\".json\"):\n",
    "        # Extract all book fields into a single formatted text using a JQ query expression.\n",
    "        # This is so that specific data from a JSON file can be effectively parsed and extracted,\n",
    "        # resulting in a clean, readable text representation that is already \"chunked\".\n",
    "        jq_schema = (\n",
    "            '.books[] | \"Title: \" + .title + '\n",
    "            '\"\\\\nAuthor: \" + .author + '\n",
    "            '\"\\\\nGenre: \" + .genre + '\n",
    "            '\"\\\\nPublication Year: \" + (.publication_year | tostring) + '\n",
    "            '\"\\\\nDescription: \" + .description + '\n",
    "            '\"\\\\nPositive Review: \" + .reviews.positive + '\n",
    "            '\"\\\\nNegative Review: \" + .reviews.negative'\n",
    "        )\n",
    "\n",
    "        loader = JSONLoader(\n",
    "            file_path=path,\n",
    "            jq_schema=jq_schema,\n",
    "            text_content=False\n",
    "        )\n",
    "\n",
    "        should_split = False\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Skipping unsupported file type: {path}\")\n",
    "\n",
    "    docs = loader.load()\n",
    "    # Assign a topic label for filtering or classification, based on filename\n",
    "    topic = assign_topic(os.path.basename(path))\n",
    "\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"source\"] = os.path.basename(path)\n",
    "        doc.metadata[\"topic\"] = topic # add topic metadata here\n",
    "\n",
    "    if should_split:\n",
    "        return splitter.split_documents(docs)\n",
    "    else:\n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a879b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinch\\Documents\\GenAI Training\\Capstone Projects (New)\\Chapter 6\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Store chunks from PDF and non-PDF files\n",
    "pdf_chunks = []\n",
    "non_pdf_chunks = []\n",
    "\n",
    "## Text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "## Embedding models:\n",
    "## Here all PDF files will use Google API embeddings, and all other files will use Hugging Face embeddings\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\", google_api_key = GOOGLE_API_KEY)\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "## Split documents into chunks and add to appropriate list\n",
    "for filename in os.listdir(\"documents\"):\n",
    "    file_path = os.path.join(\"documents\", filename)\n",
    "    chunks = load_and_split_file(file_path, text_splitter)\n",
    "\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        pdf_chunks.extend(chunks)\n",
    "    else:\n",
    "        non_pdf_chunks.extend(chunks)\n",
    "\n",
    "## Store PDFs with Google embeddings\n",
    "vector_store_pdf = Chroma.from_documents(documents = pdf_chunks,\n",
    "                                         embedding = google_embeddings,\n",
    "                                         collection_name = \"pdf-collection\",\n",
    "                                         persist_directory = os.path.join(persist_directory, \"pdf\")\n",
    "                                         )\n",
    "\n",
    "## Store non-PDFs with Hugging Face embeddings\n",
    "vector_store_non_pdf = Chroma.from_documents(documents = non_pdf_chunks,\n",
    "                                             embedding = hf_embeddings,\n",
    "                                             collection_name = \"nonpdf-collection\",\n",
    "                                             persist_directory = os.path.join(persist_directory, \"nonpdf\")\n",
    "                                             )\n",
    "\n",
    "# vector_store.persist() # not needed as of the latest version of chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d140257",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main function using RetrievalQA\n",
    "def answer_question_with_retrievalqa(query: str, filename: str = None, topic: str = None, \n",
    "                                     question_type: str = None, k = 3) -> dict:\n",
    "\n",
    "    \"\"\"\n",
    "    Answers a user query using dynamic retrieval and prompting.\n",
    "\n",
    "    - If `filename` is provided, filters chunks from that specific document.\n",
    "    - If `topic` is provided, filters chunks by topic metadata.\n",
    "    - If neither is provided, auto-detects topic using a topic classifier.\n",
    "\n",
    "    Also classifies the query as 'Factual' or 'Interpretive' to adjust the prompt.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        filename (str, optional): Name of the file to restrict retrieval to.\n",
    "        topic (str, optional): Topic label to restrict document retrieval.\n",
    "        k (int): Number of top chunks to retrieve.\n",
    "\n",
    "    Returns either:\n",
    "        - dict --> result: The LLM-generated answer.\n",
    "        - dict --> result: a warning message if no relevant documents are found.\n",
    "\n",
    "    \"\"\"\n",
    "    # Classify the question type is not explicitally provided\n",
    "    if question_type is None:\n",
    "        try:\n",
    "            # Classify the question type as either \"factual\" or \"interpretive\"\n",
    "            question_type = question_classifier_chain.invoke({\"query\": query}).strip()\n",
    "            print(f\"Question Type Detected: {question_type}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Classifier error: {e}. Defaulting to Interpretive.\")\n",
    "            question_type = \"Interpretive\" # Fallback option if an exception occurs\n",
    "\n",
    "    else:\n",
    "        # Normalize user input\n",
    "        question_type = question_type.strip().capitalize()\n",
    "        if question_type not in [\"Factual\", \"Interpretive\"]:\n",
    "            print(f\"Warning: Unsupported question_type '{question_type}'. Defaulting to Interpretive.\")\n",
    "            question_type = \"Interpretive\"\n",
    "        else:\n",
    "            print(f\"Question Type Provided: {question_type}\")\n",
    "\n",
    "    # Detect the topic of the query if not explicitally provided\n",
    "    if not filename and not topic:\n",
    "        try:\n",
    "            topic = topic_classifier_chain.invoke({\"query\": query}).strip()\n",
    "            print(f\"Inferred Topic: {topic}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Topic classifier error: {e}. Skipping topic filter.\")\n",
    "\n",
    "    # Early check for the \"Other\" topic to short-circuit retrieval, as no documents used in\n",
    "    # creating the database fall under this category\n",
    "    if topic and topic.lower() == \"other\":\n",
    "        return {\"result\": \"Your question falls under the 'Other' category, which is outside the scope of the available documents.\"}\n",
    "\n",
    "    # Build metadata filter for the retriever depending on whether a filename or\n",
    "    # topic is provided by the user (or inferrred by the LLM)\n",
    "    filter_by = None\n",
    "    if filename:\n",
    "        filter_by = {\"source\": filename}\n",
    "    elif topic:\n",
    "        filter_by = {\"topic\": topic}\n",
    "\n",
    "    # Select instruction and apply to prompt\n",
    "    instruction_text = instructions.get(question_type, instructions[\"Interpretive\"])\n",
    "    dynamic_prompt = base_prompt_template.partial(instruction=instruction_text)\n",
    "\n",
    "    # Set up retriever with an optional filter\n",
    "    search_kwargs = {\"k\": k}\n",
    "    if filter_by:\n",
    "        search_kwargs[\"filter\"] = filter_by\n",
    "\n",
    "    # Mapping each file extension to respective vector store\n",
    "    filetype_to_vector_store = {\n",
    "        \".pdf\": vector_store_pdf,       # PDF --> Technology\n",
    "        \".txt\": vector_store_non_pdf,   # TEXT --> People\n",
    "        \".html\": vector_store_non_pdf,  # HTML --> Science\n",
    "        \".json\": vector_store_non_pdf,  # JSON --> Literature\n",
    "    }\n",
    "\n",
    "    # Mapping each topic to respective vector store\n",
    "    topic_to_vector_store = {\n",
    "        \"technology\": vector_store_pdf,\n",
    "        \"science\": vector_store_non_pdf,\n",
    "        \"people\": vector_store_non_pdf,\n",
    "        \"literature\": vector_store_non_pdf,\n",
    "    }\n",
    "\n",
    "    # Determine vector store based on filename or topic\n",
    "    if filename:\n",
    "        ext = os.path.splitext(filename)[1].lower()\n",
    "        vector_store = filetype_to_vector_store.get(ext, vector_store_non_pdf)  # fallback if unknown ext\n",
    "    else:\n",
    "        topic_normalized = topic.lower() if topic else \"\"\n",
    "        vector_store = topic_to_vector_store.get(topic_normalized, vector_store_non_pdf)  # fallback\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs=search_kwargs)\n",
    "\n",
    "\n",
    "    # Relevance check: Get relevant docs\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "\n",
    "    # If no relevant documents are found in the vector store for some reason, \n",
    "    # return a message to the user\n",
    "    if not relevant_docs:\n",
    "        return {\"result\": \"Sorry, I couldn't find any documents related to your question. \"\n",
    "                          \"Please try asking something else or check the document collection \"\n",
    "                          \"to see which topics are likely covered.\"}\n",
    "\n",
    "    # Build RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=False,\n",
    "        chain_type_kwargs={\"prompt\": dynamic_prompt}\n",
    "    )\n",
    "    \n",
    "    # Execute query\n",
    "    try:\n",
    "        result = qa_chain.invoke(query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during QA chain invoke: {e}\")\n",
    "        return {\"result\": \"Sorry, I was unable to process your request due to resource constraints.\"}\n",
    "    \n",
    "\n",
    "## Helper function to ask a question and run pipeline above\n",
    "def ask_question(query: str, filename=None, topic=None, question_type=None):\n",
    "    result = answer_question_with_retrievalqa(query, filename=filename, \n",
    "                                              topic=topic, question_type=question_type)\n",
    "    return result[\"result\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f93b65",
   "metadata": {},
   "source": [
    "### Testing Various Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Detected: Factual\n",
      "Inferred Topic: People\n",
      "According to the context, Alan Turing is widely considered to be the father of theoretical computer science, and he formalised the concepts of algorithm and computation with the Turing machine.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was Alan Turing's biggest contribution to mankind?\"\n",
    "\n",
    "result = ask_question(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da39f72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Provided: Factual\n",
      "The picture that emerges from the analysis of the collected data regarding the advanced learners' use of mobile devices for learning English is relatively encouraging, and the majority of the interviewees acknowledged the positive impact of using mobile devices for English study.\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the results of the study involving mobile devices?\"\n",
    "file = \"mobile_devices.pdf\"\n",
    "question_type = \"Factual\"\n",
    "\n",
    "result = ask_question(query, filename = file, question_type=question_type)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7cfe7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Provided: Interpretive\n",
      "Artificial neural networks differ from traditional machine learning models in terms of training time and computational expense. Training a deep neural network can take several weeks, requiring significantly more computational power than traditional algorithms which typically take only minutes or hours to train. The amount of computational power needed also depends on the size of the data and the complexity of the network.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do artificial neural networks differ from traditional machine learning models?\"\n",
    "question_type = \"Interpretive\"\n",
    "topic = \"Technology\"\n",
    "\n",
    "result = ask_question(query, question_type = question_type, topic=topic)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b58353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Provided: Interpretive\n",
      "Inferred Topic: Literature\n",
      "Based on the provided context, I recommend *Life of Pi* by Yann Martel as a great adventure book. The positive review describes it as “inventive storytelling with deep philosophical themes.” Additionally, the negative review notes that the narrative style “may not appeal to everyone.”\n"
     ]
    }
   ],
   "source": [
    "query = \"Recommend me a great adventure book, and list all of the reviews found along with the author.\"\n",
    "question_type = \"Interpretive\"\n",
    "\n",
    "result = ask_question(query, question_type = question_type)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655c3f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Provided: Factual\n",
      "Inferred Topic: People\n",
      "Here’s the information about Nikola Tesla based solely on the provided context:\n",
      "\n",
      "*   Nikola Tesla was born on 10 July 1856.\n",
      "*   He died on 7 January 1943.\n",
      "*   His biggest achievement was his contributions to the design of the modern alternating current (AC) electricity supply system.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me the following things about Nikola Tesla: where and when he was born, when he died, and his biggest achievemt.\" \\\n",
    "        \"Return the output as a bullet point list.\"\n",
    "question_type = \"Factual\"\n",
    "\n",
    "result = ask_question(query, question_type = question_type)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b588659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Detected: Interpretive\n",
      "Inferred Topic: Technology\n",
      "1:1 technology refers to the phenomenon of providing students with individual devices, like laptops, to enhance their learning. Its impact is that some school districts are adopting it at high rates to help students achieve at higher levels. However, the implementation of 1:1 technology is not yet universal, and it will take many years for all students to have access to it.\n"
     ]
    }
   ],
   "source": [
    "query = \"Briefly explain what 1:1 technology is and its impact on students.\"\n",
    "\n",
    "result = ask_question(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22607053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Provided: Interpretive\n",
      "The recent Congress energy bill has several issues. It fails to address how the country will make up the gap in electricity needs without more solar, wind, and storage, and it doesn’t consider how working families will manage higher energy bills. Furthermore, both versions of the bill eliminate federal support for wind and solar energy, leading to less energy on the grid and threatening jobs and investments across the country.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are some issues with the recent Congress energy bill?\"\n",
    "file = \"climate_change.html\"\n",
    "question_type = \"Interpretive\"\n",
    "\n",
    "result = answer_question_with_retrievalqa(query, filename=file, question_type=question_type)\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bd47917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Detected: Interpretive\n",
      "Some people have said that J.K. Rowling’s books are exciting, magical, and wonderfully expand the Harry Potter universe. However, some fans felt they were darker and less suitable for younger readers.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are some things people have said about J.K. Rowling's books?\"\n",
    "topic = \"Literature\"\n",
    "\n",
    "result = answer_question_with_retrievalqa(query, topic=topic)\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38fc0246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Provided: Interpretive\n",
      "Inferred Topic: Other\n",
      "Your question falls under the 'Other' category, which is outside the scope of the available documents.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the best red wine to pair with a medium-rare steak?\"\n",
    "\n",
    "result = ask_question(query, question_type = question_type)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d999ec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Type Provided: Interpretive\n",
      "Inferred Topic: Technology\n",
      "The context does not provide information about what an IT analyst does.\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain what an IT analyst does in 1-2 sentences.\"\n",
    "question_type = \"Interpretive\"\n",
    "\n",
    "result = ask_question(query, question_type=question_type)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a9ba7",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce622b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize text splitter (adjust chunk size and overlap as needed)\n",
    "chunk_size = 800\n",
    "chunk_overlap = 100\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "## Prompt template for summarization of a document/file\n",
    "summarization_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_chunk\"],\n",
    "    template=\"Summarize the following text concisely:\\n\\n{text_chunk}\\n\\nSummary:\"\n",
    ")\n",
    "\n",
    "## Build summarization chain\n",
    "summarization_chain = summarization_prompt | llm\n",
    "\n",
    "def summarize_file(file_path: str) -> str:\n",
    "    # Load and split file into chunks using existing function from before\n",
    "    chunks = load_and_split_file(file_path, text_splitter)\n",
    "\n",
    "    # Summarize each chunk individually\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        summary = summarization_chain.invoke({\"text_chunk\": chunk.page_content})\n",
    "        summaries.append(summary.strip())\n",
    "\n",
    "    # Combine all chunk summaries into one final summary\n",
    "    combined_summary_text = \" \".join(summaries)\n",
    "    final_summary = summarization_chain.invoke({\"text_chunk\": combined_summary_text})\n",
    "\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948b8170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function summarize_file at 0x0000022628CBC360>\n"
     ]
    }
   ],
   "source": [
    "## Sample test using an HTML article in the documents folder\n",
    "file_path = \"./documents/climate_change.html\"\n",
    "summary = summarize_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c898a48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a concise summary of the text:\n",
      "\n",
      "The current US budget bill fails to address rising electricity demand, relying too heavily on fossil fuels. This, coupled with limited component supplies and proposed cuts to clean energy incentives, threatens higher energy costs, potential blackouts, and hinders the growth of renewable energy sources. **Action is needed to preserve clean energy incentives.**\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
